package graph

// This file will be automatically regenerated based on the schema, any resolver implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen version v0.17.70

import (
	"context"
	"fmt"
	"time"

	"github.com/odigos-io/odigos/api/odigos/v1alpha1"
	"github.com/odigos-io/odigos/frontend/graph/loaders"
	"github.com/odigos-io/odigos/frontend/graph/model"
	sourceutils "github.com/odigos-io/odigos/k8sutils/pkg/source"
)

// MarkedForInstrumentation is the resolver for the markedForInstrumentation field.
func (r *k8sWorkloadResolver) MarkedForInstrumentation(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadMakredForInstrumentation, error) {
	l := loaders.For(ctx)
	sources, err := l.GetSources(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	enabled, reason, err := sourceutils.IsObjectInstrumentedBySource(ctx, sources, err)
	if err != nil {
		return nil, err
	}

	return &model.K8sWorkloadMakredForInstrumentation{
		MarkedForInstrumentation: enabled,
		DecisionEnum:             string(reason.Reason),
		Message:                  reason.Message,
	}, nil
}

// RuntimeInfo is the resolver for the runtimeInfo field.
func (r *k8sWorkloadResolver) RuntimeInfo(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRuntimeInfo, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	var runtimeInfoReason *string
	var runtimeInfoMessage string = "runtime detection status not yet available"
	for _, c := range ic.Status.Conditions {
		if c.Type == v1alpha1.RuntimeDetectionStatusConditionType {
			runtimeInfoReason = &c.Reason
			runtimeInfoMessage = c.Message
		}
	}

	containers := make([]*model.K8sWorkloadRuntimeInfoContainer, len(ic.Status.RuntimeDetailsByContainer))
	for i, container := range ic.Status.RuntimeDetailsByContainer {
		containers[i] = runtimeDetailsContainersToModel(&container)
	}

	runtimeInfo := &model.K8sWorkloadRuntimeInfo{
		Completed: len(ic.Status.RuntimeDetailsByContainer) > 0,
		CompletedStatus: &model.DesiredConditionStatus{
			Name:       v1alpha1.RuntimeDetectionStatusConditionType,
			Status:     runtimeDetectionStatusCondition(runtimeInfoReason),
			ReasonEnum: runtimeInfoReason,
			Message:    runtimeInfoMessage,
		},
		Containers: containers,
	}

	return runtimeInfo, nil
}

// AgentEnabled is the resolver for the agentEnabled field.
func (r *k8sWorkloadResolver) AgentEnabled(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadAgentEnabled, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	var agentEnabledStatus *model.DesiredConditionStatus
	for _, c := range ic.Status.Conditions {
		if c.Type == v1alpha1.AgentEnabledStatusConditionType {
			conditionStatus := agentEnabledStatusCondition(&c.Reason)
			agentEnabledStatus = &model.DesiredConditionStatus{
				Name:       c.Type,
				Status:     conditionStatus,
				ReasonEnum: &c.Reason,
				Message:    c.Message,
			}
			break
		}
	}

	containers := make([]*model.K8sWorkloadAgentEnabledContainer, 0, len(ic.Spec.Containers))
	for _, container := range ic.Spec.Containers {
		containerModel := agentEnabledContainersToModel(&container)
		containers = append(containers, containerModel)
	}

	return &model.K8sWorkloadAgentEnabled{
		AgentEnabled:  ic.Spec.AgentInjectionEnabled,
		EnabledStatus: agentEnabledStatus,
		Containers:    containers,
	}, nil
}

// Rollout is the resolver for the rollout field.
func (r *k8sWorkloadResolver) Rollout(ctx context.Context, obj *model.K8sWorkload) (*model.K8sWorkloadRollout, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	var rolloutStatus *model.DesiredConditionStatus
	for _, c := range ic.Status.Conditions {
		if c.Type == v1alpha1.WorkloadRolloutStatusConditionType {
			conditionStatus := workloadRolloutStatusCondition(&c.Reason)
			rolloutStatus = &model.DesiredConditionStatus{
				Name:       c.Type,
				Status:     conditionStatus,
				ReasonEnum: &c.Reason,
				Message:    c.Message,
			}
			break
		}
	}

	if rolloutStatus == nil {
		return nil, nil
	}

	return &model.K8sWorkloadRollout{
		RolloutStatus: rolloutStatus,
	}, nil
}

// Containers is the resolver for the containers field.
func (r *k8sWorkloadResolver) Containers(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadContainer, error) {
	if obj == nil || obj.ID == nil {
		return nil, nil
	}
	l := loaders.For(ctx)
	ic, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil || ic == nil {
		return nil, err
	}

	containerByName := make(map[string]*model.K8sWorkloadContainer)
	for _, container := range ic.Spec.Containers {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].AgentEnabled = agentEnabledContainersToModel(&container)
	}

	for _, container := range ic.Status.RuntimeDetailsByContainer {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		containerByName[container.ContainerName].RuntimeInfo = runtimeDetailsContainersToModel(&container)
	}

	for _, container := range ic.Spec.ContainersOverrides {
		if _, ok := containerByName[container.ContainerName]; !ok {
			containerByName[container.ContainerName] = &model.K8sWorkloadContainer{
				ContainerName: container.ContainerName,
			}
		}
		overrides := &model.K8sWorkloadContainerOverrides{
			ContainerName: container.ContainerName,
		}
		if container.RuntimeInfo != nil {
			overrides.RuntimeInfo = runtimeDetailsContainersToModel(container.RuntimeInfo)
		}
		containerByName[container.ContainerName].Overrides = overrides
	}

	containers := make([]*model.K8sWorkloadContainer, 0, len(containerByName))
	for _, container := range containerByName {
		containers = append(containers, container)
	}

	return containers, nil
}

// Pods is the resolver for the pods field.
func (r *k8sWorkloadResolver) Pods(ctx context.Context, obj *model.K8sWorkload) ([]*model.K8sWorkloadPod, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	instrumentationConfig, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	podModels := make([]*model.K8sWorkloadPod, 0, len(pods))
	for _, pod := range pods {
		agentInjected, agentInjectedStatus := getPodAgentInjectedStatus(pod, instrumentationConfig)
		containerModels := make([]*model.K8sWorkloadPodContainer, 0, len(pod.Spec.Containers))

		// set aggregated pod health status to success and override if any container is not healthy.
		podHealthReasonStr := string(PodContainerHealthReasonHealthy)
		podHealthStatus := &model.DesiredConditionStatus{
			Name:       podHealthStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &podHealthReasonStr,
			Message:    "all containers are healthy",
		}
		for _, container := range pod.Spec.Containers {
			containerStatus := getContainerStatus(pod, container.Name)
			var started, ready *bool
			var isCrashLoop bool = false // never set to nil
			var runninsStartedTime, waitingReasonEnum, waitingMessage *string
			var restartCount *int
			if containerStatus != nil {
				started = containerStatus.Started
				ready = &containerStatus.Ready
				restartCountInt := int(containerStatus.RestartCount)
				restartCount = &restartCountInt
				if containerStatus.State.Waiting != nil {
					isCrashLoop = containerStatus.State.Waiting.Reason == "CrashLoopBackOff"
					waitingReasonEnum = &containerStatus.State.Waiting.Reason
					waitingMessage = &containerStatus.State.Waiting.Message
				}
				if containerStatus.State.Running != nil {
					runningStartedTimeStr := containerStatus.State.Running.StartedAt.Format(time.RFC3339)
					runninsStartedTime = &runningStartedTimeStr
				}
			}

			// pod container is considered healthy if it is started, ready and not in crash loop back off.
			healthStatus := &model.DesiredConditionStatus{
				Name: podContainerHealthStatus,
			}
			if isCrashLoop {
				reasonStr := string(PodContainerHealthReasonCrashLoopBackOff)

				healthStatus.Status = model.DesiredStateProgressError
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod in crash loop back off: " + *waitingReasonEnum

				podHealthStatus.Status = model.DesiredStateProgressError
				podHealthStatus.ReasonEnum = &reasonStr
				podHealthStatus.Message = "container in pod is in crash loop back off"

			} else if started == nil || !*started {
				reasonStr := string(PodContainerHealthReasonNotStarted)

				healthStatus.Status = model.DesiredStateProgressWaiting
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod not started yet"

				if podHealthStatus.Status != model.DesiredStateProgressError {
					podHealthStatus.Status = model.DesiredStateProgressWaiting
					podHealthStatus.ReasonEnum = &reasonStr
					podHealthStatus.Message = "container in pod is not started yet"
				}

			} else if ready == nil || !*ready {
				reasonStr := string(PodContainerHealthReasonNotReady)

				healthStatus.Status = model.DesiredStateProgressWaiting
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod not ready yet"

				if podHealthStatus.Status != model.DesiredStateProgressError {
					podHealthStatus.Status = model.DesiredStateProgressWaiting
					podHealthStatus.ReasonEnum = &reasonStr
					podHealthStatus.Message = "container in pod is not ready yet"
				}
			} else {
				reasonStr := string(PodContainerHealthReasonHealthy)
				healthStatus.Status = model.DesiredStateProgressSuccess
				healthStatus.ReasonEnum = &reasonStr
				healthStatus.Message = "pod is healthy"
			}

			containerModels = append(containerModels, &model.K8sWorkloadPodContainer{
				ContainerName:      container.Name,
				Started:            started,
				Ready:              ready,
				IsCrashLoop:        &isCrashLoop,
				RestartCount:       restartCount,
				RunningStartedTime: runninsStartedTime,
				WaitingReasonEnum:  waitingReasonEnum,
				WaitingMessage:     waitingMessage,
				HealthStatus:       healthStatus,
			})
		}
		podModels = append(podModels, &model.K8sWorkloadPod{
			PodName:             pod.Name,
			NodeName:            pod.Spec.NodeName,
			StartTime:           pod.CreationTimestamp.Format(time.RFC3339),
			AgentInjected:       agentInjected,
			AgentInjectedStatus: agentInjectedStatus,
			// TODO: RunningLatestWorkloadRevision
			Containers:      containerModels,
			PodHealthStatus: podHealthStatus,
		})
	}
	return podModels, nil
}

// PodsAgentInjectionStatus is the resolver for the podsAgentInjectionStatus field.
func (r *k8sWorkloadResolver) PodsAgentInjectionStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	l := loaders.For(ctx)
	pods, err := l.GetWorkloadPods(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}
	instrumentationConfig, err := l.GetInstrumentationConfig(ctx, *obj.ID)
	if err != nil {
		return nil, err
	}

	if len(pods) == 0 {
		reasonStr := string(PodsAgentInjectionReasonNoPodsAgentInjected)
		return &model.DesiredConditionStatus{
			Name:       podsAgentInjectionStatus,
			Status:     model.DesiredStateProgressDisabled,
			ReasonEnum: &reasonStr,
			Message:    "no pods found for this workload",
		}, nil
	}

	numSuccess := 0
	numNotSuccess := 0
	for _, pod := range pods {
		_, agentInjectedStatus := getPodAgentInjectedStatus(pod, instrumentationConfig)
		if agentInjectedStatus.Status == model.DesiredStateProgressSuccess {
			numSuccess++
		} else {
			numNotSuccess++
		}
	}

	// if instrumentationConfig is nil, we assume agent is not enabled.
	agentEnabled := false
	if instrumentationConfig != nil {
		agentEnabled = instrumentationConfig.Spec.AgentInjectionEnabled
	}

	if numSuccess == 0 && numNotSuccess > 0 {
		var reasonStr, message string
		if agentEnabled {
			reasonStr = string(PodsAgentInjectionReasonSomePodsAgentInjected)
			message = fmt.Sprintf("%d/%d pods have agent injected when it should not", numNotSuccess, len(pods))
		} else {
			reasonStr = string(PodsAgentInjectionReasonSomePodsAgentNotInjected)
			message = fmt.Sprintf("%d/%d pods do not have agent injected when it should", numNotSuccess, len(pods))
		}
		return &model.DesiredConditionStatus{
			Name:       podsAgentInjectionStatus,
			Status:     model.DesiredStateProgressWaiting,
			ReasonEnum: &reasonStr,
			Message:    message,
		}, nil
	} else {
		var reasonStr, message string
		if agentEnabled {
			reasonStr = string(PodsAgentInjectionReasonAllPodsAgentInjected)
			message = fmt.Sprintf("all %d pods have odigos agent injected as expected", numSuccess)
		} else {
			reasonStr = string(PodsAgentInjectionReasonAllPodsAgentNotInjected)
			message = fmt.Sprintf("all %d pods do not have odigos agent injected as expected", numSuccess)
		}
		return &model.DesiredConditionStatus{
			Name:       podsAgentInjectionStatus,
			Status:     model.DesiredStateProgressSuccess,
			ReasonEnum: &reasonStr,
			Message:    message,
		}, nil
	}
}

// PodsHealthStatus is the resolver for the podsHealthStatus field.
func (r *k8sWorkloadResolver) PodsHealthStatus(ctx context.Context, obj *model.K8sWorkload) (*model.DesiredConditionStatus, error) {
	panic(fmt.Errorf("not implemented: PodsHealthStatus - podsHealthStatus"))
}

// K8sWorkload returns K8sWorkloadResolver implementation.
func (r *Resolver) K8sWorkload() K8sWorkloadResolver { return &k8sWorkloadResolver{r} }

type k8sWorkloadResolver struct{ *Resolver }
